{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08fccf46",
   "metadata": {
    "id": "08fccf46"
   },
   "source": [
    "# Logical Transduction\n",
    "\n",
    "Agentics objects are capable of performing logical transduction between their states by using the provied LLMs. The logical transduction operator << can be applied between any two agentics of any type.\n",
    "\n",
    "Transduction is done between a source and a target AG when connected by the << operator.\n",
    "\n",
    "The follwing example transduces a Question into an Answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069407b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3284,
     "status": "ok",
     "timestamp": 1757074891021,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "069407b8",
    "outputId": "df336c0c-62bb-40f9-9132-2bb38f6c0c2a"
   },
   "outputs": [],
   "source": [
    "# ! uv pip install agentics-py\n",
    "\n",
    "! uv pip install \"git+https://github.com/IBM/agentics/@Colab\"\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "CURRENT_PATH = \"\"\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "print(\"In Colab:\", IN_COLAB)\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "print(\"In Colab:\", IN_COLAB)\n",
    "\n",
    "if IN_COLAB:\n",
    "    CURRENT_PATH = \"/content/drive/MyDrive/\"\n",
    "    # Mount your google drive\n",
    "    load_dotenv(\"/content/drive/MyDrive/.env\")\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    from google.colab import userdata\n",
    "\n",
    "    os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
    "else:\n",
    "    load_dotenv(find_dotenv())\n",
    "\n",
    "if not os.getenv(\"GEMINI_API_KEY\"):\n",
    "    os.environ[\"GEMINI_API_KEY\"] = input(\"Enter your GEMINI_API_KEY:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b080d14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "333bba1a5012475a946a9d10ae2525fc",
      "958824c60e6a422cbe77d7dc95cbed16"
     ]
    },
    "executionInfo": {
     "elapsed": 14872,
     "status": "ok",
     "timestamp": 1757074905896,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "7b080d14",
    "outputId": "5d7382b0-6357-4a97-f7cd-d362152e0fd1"
   },
   "outputs": [],
   "source": [
    "from agentics import Agentics as AG\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from agentics.core.llm_connections import get_llm_provider\n",
    "\n",
    "\n",
    "## Define target and source types\n",
    "class Answer(BaseModel):\n",
    "    answer: Optional[str] = None\n",
    "    justification: Optional[str] = None\n",
    "    confidence: Optional[float] = None\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: Optional[str] = None\n",
    "\n",
    "\n",
    "## Instantiate the source AG with a question\n",
    "source = AG(\n",
    "    atype=Question,\n",
    "    llm=get_llm_provider(),  ## You can choose between \"openai\" (i.e. get_llm_provider(\"openai\")), \"watsonx\", \"gemini\", \"vllm_crewai\"\n",
    "    ## set verbose to true to see the internal agents log. This is optional.\n",
    "    states=[Question(question=\"What is the capital of Italy?\")],\n",
    ")\n",
    "\n",
    "## Instantiate the target AG with a target type. No instances are needed for zero shot transduction\n",
    "target = AG(\n",
    "    atype=Answer,  ## You can choose between \"openai\", \"watsonx\", \"gemini\", \"vllm_crewai\"\n",
    "    verbose_agent=True,\n",
    ")  ## set verbose to true to see the internal agents log\n",
    "\n",
    "# Execute logical transduction by using the << operator between source and target AG\n",
    "answer = await (\n",
    "    target << source\n",
    ")  ## Note that << operator is asyncronus and the results should be awaited\n",
    "\n",
    "# Print the results of the transduction\n",
    "print(\n",
    "    answer.pretty_print()\n",
    ")  ## Note that confidence is a float number, while other fields are strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8681fea",
   "metadata": {
    "id": "b8681fea"
   },
   "source": [
    "In agentics, lists of strings can be used as sources instead of AG. Those are provided as input for the transduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f8d2e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1595,
     "status": "ok",
     "timestamp": 1757074907494,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "66f8d2e0",
    "outputId": "a04db6b5-016b-4c11-d374-f5f9e62dc731"
   },
   "outputs": [],
   "source": [
    "answer = await (AG(atype=Answer) << [\"Where is Paris?\"])\n",
    "answer.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade51f3",
   "metadata": {
    "id": "2ade51f3"
   },
   "source": [
    "## Asyncronous Transduction\n",
    "\n",
    "When the source AG has more than one state, the << operator perform asyncronous transfuction for each state to the target type. Transductions are executed in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec8966",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8082,
     "status": "ok",
     "timestamp": 1757074915577,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "b4ec8966",
    "outputId": "da8b8411-2788-4a5e-d616-f83e41cee8d2"
   },
   "outputs": [],
   "source": [
    "target = AG(\n",
    "    atype=Answer,\n",
    "    verbose_transduction=True,  # Set to verbose to see transduction timings and other logs\n",
    "    transduction_logs_path=\"/tmp/answers.jsonl\",\n",
    ")  # Optionally write longs of transductions on the specified path\n",
    "questions = [\n",
    "    \"Where is Paris?\",\n",
    "    \"Who is Alberto Sordi\",\n",
    "    \"When will climate change be irreverible?\",\n",
    "    \"Who is the best Jeopardy player?\",\n",
    "]\n",
    "\n",
    "answers = await (target << questions)\n",
    "\n",
    "answer.pretty_print()\n",
    "\n",
    "answers = await (target << questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298be399",
   "metadata": {
    "id": "298be399"
   },
   "source": [
    "## Self Transduction\n",
    "\n",
    "Self transduction is a method of AGs that enables async execution of transductions between slots of the same object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974143b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38722,
     "status": "ok",
     "timestamp": 1757075164819,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "974143b6",
    "outputId": "0f63dbf4-462d-4be7-8ed5-e6ca62af16aa"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "## Define the Pydantic type\n",
    "class Movie(BaseModel):\n",
    "    movie_name: Optional[str] = (\n",
    "        None  ## Note that fields name should match the column name in the input csv\n",
    "    )\n",
    "    genre: Optional[str] = None\n",
    "    description: Optional[str] = None\n",
    "    tweet: Optional[str] = Field(\n",
    "        None, description=\"Generate a Tweet to advertise the movie\"\n",
    "    )\n",
    "\n",
    "\n",
    "base = Path(CURRENT_PATH)\n",
    "movies = AG.from_csv(\n",
    "    base / \"data/movies.csv\", atype=Movie, max_rows=20\n",
    ")  ## Load the input data from a csv file\n",
    "movies.verbose_transduction = True\n",
    "movies.llm = get_llm_provider(\n",
    "    \"watsonx\"\n",
    ")  ## You can choose between \"openai\", \"watsonx\", \"gemini\", \"vllm_crewai\"\n",
    "\n",
    "movies_with_tweets = await movies.self_transduction(\n",
    "    [\"movie_name\", \"genre\", \"description\"],  ## source fields\n",
    "    [\"tweet\"],  ## target fields\n",
    "    ## Note that instruction are only needed when the  relation between source and target type\n",
    "    # is not innediately clear and need to be further specified or disambiguated.\n",
    "    instructions=\"Generate a tweet to advertise the release of the input movie\",\n",
    ")\n",
    "\n",
    "movies_with_tweets.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc007db0",
   "metadata": {
    "id": "fc007db0"
   },
   "source": [
    "As an alternative, self transduction can be encoded using logical transduction algebra which uses the AG() notation to rebind the original data into AGs of the requested subtypes. Learn more about atype manipulation in agentics [here](link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb1b7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38963,
     "status": "ok",
     "timestamp": 1757075203780,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "8dbb1b7f",
    "outputId": "d9c0de9a-56ba-498e-f0e9-051b5c78cec9"
   },
   "outputs": [],
   "source": [
    "movies = AG.from_csv(base / \"data/movies.csv\", atype=Movie, max_rows=20)\n",
    "tweets = await (\n",
    "    AG(\n",
    "        atype=movies(\"tweet\").atype,\n",
    "        instructions=\"Generate a tweet to advertise the release of the input movie\",\n",
    "    )\n",
    "    << movies(\"movie_name\", \"genre\", \"description\")\n",
    ")\n",
    "print(\n",
    "    tweets.pretty_print()\n",
    ")  ## Note that differently from self transduction the output tweets\n",
    "##has only the tweet field, whereas self transduction preserves\n",
    "## the original source data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3044a80",
   "metadata": {
    "id": "c3044a80"
   },
   "source": [
    "## Few Shots Transduction\n",
    "\n",
    "Few shots examples can be provided for transduction by adding instances of the target instances in correspondance to their sources . Those will be used by the LLM to infer by analogy all the Null instances of the target type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4803bba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7671,
     "status": "ok",
     "timestamp": 1757075243259,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "a4803bba",
    "outputId": "ca59e4dc-a35e-4aec-c823-37ab94ef94b4"
   },
   "outputs": [],
   "source": [
    "movies = AG.from_csv(base / \"data/movies.csv\", max_rows=20)\n",
    "## Note that obly the first 9 movies have categories\n",
    "for i, movie in enumerate(movies):\n",
    "    print(f\"{i}: {movie.genre}\")\n",
    "## predicting new genre from given examples\n",
    "all_genres = await (movies(\"genre\") << movies(\"movie_name\", \"description\"))\n",
    "all_genres.pretty_print()\n",
    "## few shots examples can also be used in self transfuction\n",
    "movies_with_genre = await movies.self_transduction(\n",
    "    [\"movie_name\", \"description\"], [\"genre\"]\n",
    ")\n",
    "print(movies_with_genre.pretty_print())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/IBM/Agentics/blob/Colab/tutorials/transduction.ipynb",
     "timestamp": 1757075262425
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "333bba1a5012475a946a9d10ae2525fc": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_958824c60e6a422cbe77d7dc95cbed16",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸš€ Crew: crew</span>\nâ””â”€â”€ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ðŸ“‹ Task: 9a635803-1e07-4de4-a5df-3e332d7e7051</span>\n    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n</pre>\n",
         "text/plain": "\u001b[1;36mðŸš€ Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\nâ””â”€â”€ \u001b[1;33mðŸ“‹ Task: 9a635803-1e07-4de4-a5df-3e332d7e7051\u001b[0m\n    \u001b[37mStatus: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "958824c60e6a422cbe77d7dc95cbed16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
